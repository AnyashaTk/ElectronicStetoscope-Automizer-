{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silero VAD + webrtcvad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting silero-vad\n",
      "  Downloading silero_vad-5.1.2-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting onnxruntime>=1.16.1 (from silero-vad)\n",
      "  Downloading onnxruntime-1.22.0-cp311-cp311-macosx_13_0_universal2.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: torch>=1.12.0 in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from silero-vad) (2.7.0)\n",
      "Requirement already satisfied: torchaudio>=0.12.0 in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from silero-vad) (2.7.0)\n",
      "Collecting coloredlogs (from onnxruntime>=1.16.1->silero-vad)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from onnxruntime>=1.16.1->silero-vad) (25.2.10)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from onnxruntime>=1.16.1->silero-vad) (2.1.3)\n",
      "Requirement already satisfied: packaging in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from onnxruntime>=1.16.1->silero-vad) (24.2)\n",
      "Requirement already satisfied: protobuf in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from onnxruntime>=1.16.1->silero-vad) (5.29.3)\n",
      "Requirement already satisfied: sympy in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from onnxruntime>=1.16.1->silero-vad) (1.14.0)\n",
      "Requirement already satisfied: filelock in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from torch>=1.12.0->silero-vad) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from torch>=1.12.0->silero-vad) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from torch>=1.12.0->silero-vad) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from torch>=1.12.0->silero-vad) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from torch>=1.12.0->silero-vad) (2025.3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from sympy->onnxruntime>=1.16.1->silero-vad) (1.3.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.16.1->silero-vad)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from jinja2->torch>=1.12.0->silero-vad) (3.0.2)\n",
      "Downloading silero_vad-5.1.2-py3-none-any.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-macosx_13_0_universal2.whl (34.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime, silero-vad\n",
      "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.22.0 silero-vad-5.1.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install silero-vad # pyAudioAnalysis # webrtcvad # torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import webrtcvad\n",
    "import soundfile as sf\n",
    "from scipy.signal import butter, lfilter \n",
    "import scipy.fft\n",
    "import librosa\n",
    "\n",
    "from silero_vad import load_silero_vad, read_audio, get_speech_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка модели Silero VAD\n",
    "model = load_silero_vad()\n",
    "\n",
    "def remove_speech_silero_vad(audio_file):\n",
    "    # Чтение аудио\n",
    "    audio, sample_rate = sf.read(audio_file)\n",
    "    audio = librosa.to_mono(audio)\n",
    "    # Использование Silero VAD\n",
    "    speech_timestamps = get_speech_timestamps(audio, model)  # Return speech timestamps in seconds (default is samples))\n",
    "    print(speech_timestamps)\n",
    "    # Вырезаем участки с речью\n",
    "    speech_free_audio = []\n",
    "    for start, stop in speech_timestamps:\n",
    "        speech_free_audio.append(audio[start:stop])\n",
    "\n",
    "    speech_free_audio = np.concatenate(speech_free_audio, axis=0)\n",
    "    return speech_free_audio, sample_rate\n",
    "\n",
    "def remove_speech_webrtcvad(audio_file, sample_rate):\n",
    "    # Инициализация webrtcvad\n",
    "    vad = webrtcvad.Vad(1)  # Порог чувствительности 1 (низкий)\n",
    "    audio, sample_rate = sf.read(audio_file)\n",
    "    frame_duration = 0.02  # 20 ms\n",
    "    frame_size = int(sample_rate * frame_duration)\n",
    "    frames = [audio[i:i+frame_size] for i in range(0, len(audio), frame_size)]\n",
    "    speech_free_audio = []\n",
    "\n",
    "    for frame in frames:\n",
    "        if vad.is_speech(frame.tobytes(), sample_rate):\n",
    "            continue  # Пропускаем фрейм с речью\n",
    "        speech_free_audio.append(frame)\n",
    "\n",
    "    return np.concatenate(speech_free_audio, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Основной пайплайн\u001b[39;00m\n\u001b[32m      2\u001b[39m audio_file = \u001b[33m\"\u001b[39m\u001b[33m../data/норма/Соболева Ирина Анатольевна.wav\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m speech_free_audio, sample_rate = \u001b[43mremove_speech_silero_vad\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m clean_audio = remove_speech_webrtcvad(audio_file, sample_rate)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Сохранение очищенного аудио\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mremove_speech_silero_vad\u001b[39m\u001b[34m(audio_file)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m start, stop \u001b[38;5;129;01min\u001b[39;00m speech_timestamps:\n\u001b[32m     14\u001b[39m     speech_free_audio.append(audio[start:stop])\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m speech_free_audio = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspeech_free_audio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m speech_free_audio, sample_rate\n",
      "\u001b[31mValueError\u001b[39m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "\n",
    "# Основной пайплайн\n",
    "audio_file = \"../data/норма/Соболева Ирина Анатольевна.wav\"\n",
    "speech_free_audio, sample_rate = remove_speech_silero_vad(audio_file)\n",
    "clean_audio = remove_speech_webrtcvad(audio_file, sample_rate)\n",
    "\n",
    "# Сохранение очищенного аудио\n",
    "sf.write(\"../data/норма/Соболева Ирина Анатольевна_cleaned.wav\", clean_audio, sample_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spleeter или Meta Voice Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.7.0 (from versions: 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.7.0\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spleeter'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspleeter\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mseparator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Separator\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msoundfile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msf\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Инициализация Spleeter для разделения звуков на два источника (вокал и аккомпанемент)\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'spleeter'"
     ]
    }
   ],
   "source": [
    "from spleeter.separator import Separator\n",
    "import soundfile as sf\n",
    "\n",
    "# Инициализация Spleeter для разделения звуков на два источника (вокал и аккомпанемент)\n",
    "separator = Separator('spleeter:2stems')\n",
    "\n",
    "# Разделение аудио\n",
    "separator.separate_to_file('../data/норма/Соболева Ирина Анатольевна.wav', './')\n",
    "\n",
    "# Загрузка чистых звуков (например, аккомпанемент)\n",
    "clean_audio, sample_rate = sf.read('./Соболева Ирина Анатольевна.wav')\n",
    "\n",
    "# Сохранение очищенного аудио\n",
    "sf.write('cleaned_auscultation_no_speech.wav', clean_audio, sample_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openunmix\n",
      "  Downloading openunmix-1.3.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from openunmix) (2.1.3)\n",
      "Requirement already satisfied: torchaudio>=0.9.0 in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from openunmix) (2.7.0)\n",
      "Requirement already satisfied: torch>=1.9.0 in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from openunmix) (2.7.0)\n",
      "Requirement already satisfied: tqdm in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from openunmix) (4.67.1)\n",
      "Requirement already satisfied: filelock in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from torch>=1.9.0->openunmix) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from torch>=1.9.0->openunmix) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from torch>=1.9.0->openunmix) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from torch>=1.9.0->openunmix) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from torch>=1.9.0->openunmix) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from torch>=1.9.0->openunmix) (2025.3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.9.0->openunmix) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from jinja2->torch>=1.9.0->openunmix) (3.0.2)\n",
      "Downloading openunmix-1.3.0-py3-none-any.whl (40 kB)\n",
      "Installing collected packages: openunmix\n",
      "Successfully installed openunmix-1.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install openunmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['input_mean', 'input_scale', 'output_scale', 'output_mean', 'sample_rate', 'stft.window', 'transform.0.window', 'fc1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lstm.weight_ih_l0_reverse', 'lstm.weight_hh_l0_reverse', 'lstm.bias_ih_l0_reverse', 'lstm.bias_hh_l0_reverse', 'lstm.weight_ih_l1', 'lstm.weight_hh_l1', 'lstm.bias_ih_l1', 'lstm.bias_hh_l1', 'lstm.weight_ih_l1_reverse', 'lstm.weight_hh_l1_reverse', 'lstm.bias_ih_l1_reverse', 'lstm.bias_hh_l1_reverse', 'lstm.weight_ih_l2', 'lstm.weight_hh_l2', 'lstm.bias_ih_l2', 'lstm.bias_hh_l2', 'lstm.weight_ih_l2_reverse', 'lstm.weight_hh_l2_reverse', 'lstm.bias_ih_l2_reverse', 'lstm.bias_hh_l2_reverse', 'fc2.weight', 'bn2.weight', 'bn2.bias', 'bn2.running_mean', 'bn2.running_var', 'bn2.num_batches_tracked', 'fc3.weight', 'bn3.weight', 'bn3.bias', 'bn3.running_mean', 'bn3.running_var', 'bn3.num_batches_tracked'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for OpenUnmix:\n\tsize mismatch for input_mean: copying a param with shape torch.Size([1487]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for input_scale: copying a param with shape torch.Size([1487]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for output_scale: copying a param with shape torch.Size([2049]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for output_mean: copying a param with shape torch.Size([2049]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([1024, 2974]) from checkpoint, the shape in current model is torch.Size([512, 8192]).\n\tsize mismatch for bn1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for bn1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for bn1.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for bn1.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([2048, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.weight_ih_l0_reverse: copying a param with shape torch.Size([2048, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for lstm.weight_hh_l0_reverse: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for lstm.bias_ih_l0_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.bias_hh_l0_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([2048, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.weight_ih_l1_reverse: copying a param with shape torch.Size([2048, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for lstm.weight_hh_l1_reverse: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for lstm.bias_ih_l1_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.bias_hh_l1_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.weight_ih_l2: copying a param with shape torch.Size([2048, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for lstm.weight_hh_l2: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for lstm.bias_ih_l2: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.bias_hh_l2: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.weight_ih_l2_reverse: copying a param with shape torch.Size([2048, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for lstm.weight_hh_l2_reverse: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for lstm.bias_ih_l2_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.bias_hh_l2_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([1024, 2048]) from checkpoint, the shape in current model is torch.Size([512, 1024]).\n\tsize mismatch for bn2.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for bn2.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for bn2.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for bn2.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for fc3.weight: copying a param with shape torch.Size([4098, 1024]) from checkpoint, the shape in current model is torch.Size([8192, 512]).\n\tsize mismatch for bn3.weight: copying a param with shape torch.Size([4098]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for bn3.bias: copying a param with shape torch.Size([4098]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for bn3.running_mean: copying a param with shape torch.Size([4098]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for bn3.running_var: copying a param with shape torch.Size([4098]) from checkpoint, the shape in current model is torch.Size([8192]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     26\u001b[39m filtered_state_dict = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m checkpoint.items() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m     27\u001b[39m     skip \u001b[38;5;129;01min\u001b[39;00m k \u001b[38;5;28;01mfor\u001b[39;00m skip \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33msample_rate\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstft.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtransform.0.\u001b[39m\u001b[33m\"\u001b[39m])}\n\u001b[32m     29\u001b[39m umx = OpenUnmix()\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[43mumx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_state_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m umx.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2593\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2585\u001b[39m         error_msgs.insert(\n\u001b[32m   2586\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2587\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2589\u001b[39m             ),\n\u001b[32m   2590\u001b[39m         )\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2594\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2596\u001b[39m         )\n\u001b[32m   2597\u001b[39m     )\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for OpenUnmix:\n\tsize mismatch for input_mean: copying a param with shape torch.Size([1487]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for input_scale: copying a param with shape torch.Size([1487]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for output_scale: copying a param with shape torch.Size([2049]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for output_mean: copying a param with shape torch.Size([2049]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([1024, 2974]) from checkpoint, the shape in current model is torch.Size([512, 8192]).\n\tsize mismatch for bn1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for bn1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for bn1.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for bn1.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([2048, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.weight_ih_l0_reverse: copying a param with shape torch.Size([2048, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for lstm.weight_hh_l0_reverse: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for lstm.bias_ih_l0_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.bias_hh_l0_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([2048, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.weight_ih_l1_reverse: copying a param with shape torch.Size([2048, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for lstm.weight_hh_l1_reverse: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for lstm.bias_ih_l1_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.bias_hh_l1_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.weight_ih_l2: copying a param with shape torch.Size([2048, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for lstm.weight_hh_l2: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for lstm.bias_ih_l2: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.bias_hh_l2: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.weight_ih_l2_reverse: copying a param with shape torch.Size([2048, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for lstm.weight_hh_l2_reverse: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for lstm.bias_ih_l2_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.bias_hh_l2_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([1024, 2048]) from checkpoint, the shape in current model is torch.Size([512, 1024]).\n\tsize mismatch for bn2.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for bn2.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for bn2.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for bn2.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for fc3.weight: copying a param with shape torch.Size([4098, 1024]) from checkpoint, the shape in current model is torch.Size([8192, 512]).\n\tsize mismatch for bn3.weight: copying a param with shape torch.Size([4098]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for bn3.bias: copying a param with shape torch.Size([4098]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for bn3.running_mean: copying a param with shape torch.Size([4098]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for bn3.running_var: copying a param with shape torch.Size([4098]) from checkpoint, the shape in current model is torch.Size([8192])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "import os\n",
    "from openunmix.model import OpenUnmix\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "# === ПАРАМЕТРЫ ===\n",
    "AUDIO_PATH = \"../data/норма/Соболева Ирина Анатольевна.wav\"\n",
    "MODEL_PATH = \"/Users/annatekuceva/.cache/torch/hub/checkpoints/vocals-bccbd9aa.pth\"\n",
    "OUTPUT_PATH = \"background_only.wav\"\n",
    "\n",
    "# === ЗАГРУЗКА АУДИО ===\n",
    "# OpenUnmix работает на 44.1 kHz, stereo\n",
    "waveform, sr = librosa.load(AUDIO_PATH, sr=44100, mono=False)\n",
    "if waveform.ndim == 1:\n",
    "    waveform = np.expand_dims(waveform, axis=0)  # из mono в [1, n]\n",
    "elif waveform.shape[0] != 2:\n",
    "    raise ValueError(\"Audio must be stereo (2 channels)\")\n",
    "\n",
    "waveform = torch.tensor(waveform, dtype=torch.float32)\n",
    "\n",
    "# === ЗАГРУЗКА МОДЕЛИ ИЗ ВЕСОВ ===\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=\"cpu\")\n",
    "filtered_state_dict = {k: v for k, v in checkpoint.items() if not any(\n",
    "    skip in k for skip in [\"sample_rate\", \"stft.\", \"transform.0.\"])}\n",
    "\n",
    "umx = OpenUnmix()\n",
    "\n",
    "umx.load_state_dict(filtered_state_dict)\n",
    "umx.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === STFT ===\n",
    "n_fft = 4096\n",
    "hop_length = 1024\n",
    "win_length = 4096\n",
    "window = torch.hann_window(win_length)\n",
    "\n",
    "# Преобразуем в частотную область\n",
    "X = torch.stft(waveform, n_fft=n_fft, hop_length=hop_length, win_length=win_length,\n",
    "               window=window, return_complex=True)\n",
    "\n",
    "# === ПРЯМОЕ ПРИМЕНЕНИЕ МОДЕЛИ ===\n",
    "with torch.no_grad():\n",
    "    X_mag = X.abs().permute(1, 0, 2)  # (freq, channel, time) -> (time, channel, freq)\n",
    "    X_mag = X_mag.numpy()\n",
    "    estimates = umx(torch.tensor(X_mag)).detach().numpy()  # [T, C, F]\n",
    "\n",
    "# Восстанавливаем фазу и обратно в time-domain\n",
    "estimates = torch.tensor(estimates).permute(1, 2, 0)  # -> [C, F, T]\n",
    "estimates = torch.view_as_complex(estimates)  # [C, F, T] -> complex\n",
    "background_stft = estimates\n",
    "background_audio = torch.istft(background_stft, n_fft=n_fft, hop_length=hop_length,\n",
    "                               win_length=win_length, window=window, length=waveform.shape[1])\n",
    "\n",
    "# === СОХРАНЕНИЕ ===\n",
    "background_audio_np = background_audio.numpy()\n",
    "sf.write(OUTPUT_PATH, background_audio_np.T, sr)\n",
    "\n",
    "print(f\"Сохранено безречевое аудио в: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wavelet \\ noisereduce \\ banda stop filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting noisereduce\n",
      "  Downloading noisereduce-3.0.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: scipy in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from noisereduce) (1.15.1)\n",
      "Requirement already satisfied: matplotlib in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from noisereduce) (3.10.3)\n",
      "Requirement already satisfied: numpy in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from noisereduce) (2.1.3)\n",
      "Requirement already satisfied: tqdm in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from noisereduce) (4.67.1)\n",
      "Requirement already satisfied: joblib in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from noisereduce) (1.4.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from matplotlib->noisereduce) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from matplotlib->noisereduce) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from matplotlib->noisereduce) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from matplotlib->noisereduce) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from matplotlib->noisereduce) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from matplotlib->noisereduce) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from matplotlib->noisereduce) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from matplotlib->noisereduce) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->noisereduce) (1.17.0)\n",
      "Downloading noisereduce-3.0.3-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: noisereduce\n",
      "Successfully installed noisereduce-3.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install noisereduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annatekuceva/PycharmProjects/stetoscope/ElectronicStetoscope-Automizer-/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "import noisereduce as nr\n",
    "import numpy as np\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path):\n",
    "    \"\"\"Загрузка аудиофайла\"\"\"\n",
    "    audio, sr = librosa.load(path, sr=None, mono=True)\n",
    "    return audio, sr\n",
    "\n",
    "def remove_voice(audio, sr, voice_freq_range=(85, 255)):\n",
    "    \"\"\"Удаление голоса (основной диапазон частот человеческого голоса)\"\"\"\n",
    "    # Создаем фильтр Банда-стоп для диапазона голоса\n",
    "    nyquist = sr / 2.0\n",
    "    low = voice_freq_range[0] / nyquist\n",
    "    high = voice_freq_range[1] / nyquist\n",
    "    \n",
    "    # Применяем фильтр\n",
    "    filtered = librosa.effects.preemphasis(audio)\n",
    "    filtered = librosa.decompose.nn_filter(\n",
    "        filtered,\n",
    "        aggregate=np.median,\n",
    "        metric='cosine',\n",
    "        width=int(librosa.time_to_frames(0.1, sr=sr)))\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "def wavelet_denoise(audio, wavelet='db4', level=1):\n",
    "    \"\"\"Вейвлет-дениоинг для удаления резких шумов\"\"\"\n",
    "    coeff = pywt.wavedec(audio, wavelet, mode=\"per\")\n",
    "    sigma = (1/0.6745) * np.median(np.abs(coeff[-level] - np.median(coeff[-level])))\n",
    "    uthresh = sigma * np.sqrt(2 * np.log(len(audio)))\n",
    "    coeff[1:] = (pywt.threshold(i, value=uthresh, mode=\"soft\") for i in coeff[1:])\n",
    "    return pywt.waverec(coeff, wavelet, mode=\"per\")\n",
    "\n",
    "def reduce_noise(audio, sr, stationary=False):\n",
    "    \"\"\"Подавление шумов с помощью noisereduce\"\"\"\n",
    "    return nr.reduce_noise(\n",
    "        y=audio,\n",
    "        sr=sr,\n",
    "        stationary=stationary,\n",
    "        prop_decrease=1.0,\n",
    "        n_fft=1024,\n",
    "        win_length=512\n",
    "    )\n",
    "\n",
    "def process_heart_sound(input_path, output_path):\n",
    "    \"\"\"Основной процесс обработки\"\"\"\n",
    "    print('1. Загрузка аудио')\n",
    "    audio, sr = load_audio(input_path)\n",
    "    \n",
    "    print('2. Удаление голоса')\n",
    "    audio = remove_voice(audio, sr)\n",
    "    \n",
    "    print('3. Подавление шумов')\n",
    "    audio = reduce_noise(audio, sr, stationary=True)\n",
    "    \n",
    "    print('4. Вейвлет-дениоинг')\n",
    "    audio = wavelet_denoise(audio)\n",
    "    \n",
    "    print('5. Нормализация')\n",
    "    audio = librosa.util.normalize(audio)\n",
    "    \n",
    "    print('6. Сохранение результата')\n",
    "    sf.write(output_path, audio, sr, subtype='PCM_16')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "import noisereduce as nr\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "\n",
    "def load_audio(path, target_sr=22050):\n",
    "    \"\"\"Загрузка с понижением частоты дискретизации для ускорения\"\"\"\n",
    "    audio, sr = librosa.load(path, sr=target_sr, mono=True)\n",
    "    return audio, sr\n",
    "\n",
    "def remove_voice_fast(audio, sr, voice_range=(85, 255)):\n",
    "    \"\"\"Быстрый банда-стоп фильтр вместо nn_filter\"\"\"\n",
    "    nyquist = sr / 2\n",
    "    low = voice_range[0] / nyquist\n",
    "    high = voice_range[1] / nyquist\n",
    "    b, a = signal.butter(4, [low, high], btype='bandstop')\n",
    "    return signal.filtfilt(b, a, audio)\n",
    "\n",
    "def wavelet_denoise(audio, wavelet='db4', level=1):\n",
    "    \"\"\"Вейвлет-дениоинг для удаления резких шумов\"\"\"\n",
    "    coeff = pywt.wavedec(audio, wavelet, mode=\"per\")\n",
    "    sigma = (1/0.6745) * np.median(np.abs(coeff[-level] - np.median(coeff[-level])))\n",
    "    uthresh = sigma * np.sqrt(2 * np.log(len(audio)))\n",
    "    coeff[1:] = (pywt.threshold(i, value=uthresh, mode=\"soft\") for i in coeff[1:])\n",
    "    return pywt.waverec(coeff, wavelet, mode=\"per\")\n",
    "\n",
    "\n",
    "def process_heart_sound_optimized(input_path, output_path):\n",
    "    audio, sr = load_audio(input_path)\n",
    "    \n",
    "    # audio = wavelet_denoise(audio)\n",
    "\n",
    "    # 1. Удаление голоса (замена nn_filter на БИХ-фильтр)\n",
    "    # audio = remove_voice_fast(audio, sr)\n",
    "    \n",
    "    # 2. Подавление шумов с увеличенным n_fft\n",
    "    audio = nr.reduce_noise(\n",
    "        y=audio, \n",
    "        sr=sr,\n",
    "        stationary=True,\n",
    "        n_fft=2048,  # Больше разрешение для низких частот\n",
    "        win_length=1024\n",
    "    )\n",
    "    \n",
    "    # 3. Сохранение\n",
    "    sf.write(output_path, audio, sr, subtype='PCM_16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработка завершена. Результат сохранен в cleaned_noisereduce.wav\n"
     ]
    }
   ],
   "source": [
    "input_file = \"../data/норма/Соболева Ирина Анатольевна.wav\"  # Входной файл\n",
    "output_file = \"cleaned_noisereduce.wav\"  # Выходной файл\n",
    "\n",
    "process_heart_sound_optimized(input_file, output_file)\n",
    "print(f\"Обработка завершена. Результат сохранен в {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
